---
title: "p8105_hw3_lg3158"
author: "Ling"
date: "10/17/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  out.width = "90%")
library(tidyverse)
library(p8105.datasets)
library(knitr)
```

# Problem 1

```{r}
data("instacart")
```

- A short description of the dataset:

  - The dataset `instacart` has `r ncol(instacart)` variables and `r nrow(instacart)` observations. The dataset includes `r nlevels(as_factor(pull(instacart, department)))` departments and `r nlevels(as_factor(pull(instacart, aisle)))` categories of products. The brief summary of this dataset is below.

```{r}
skimr::skim(instacart)
```

- How many aisles are there, and which aisles are the most items ordered from?
  - Make a summary table that:
    - include 3 variables: aisle, items_sold (the total count of the items in the aisle), sell_rank (from the most to the least sell)
    - the table is arranged by the sell rank in a descending order (from the 1st to the last)
    - present the top 5 most ordered aisles in the table

```{r}

instacart_summary = instacart %>%
  group_by(aisle) %>%
  summarise(items_sold = n()) %>%
  mutate( sell_rank = min_rank(desc(items_sold))) %>%
  arrange(sell_rank)

kable(instacart_summary[1:5, ], caption = "the top 5 most ordered aisles")
```

- From the summary and the table above, we can see that:
  - There are `r nlevels(as_factor(pull(instacart,aisle)))` aisles;
  - The fresh vegetable, fresh fruits, packaged vegetable fruits, yogurt, and packaged cheese are the aisles that most items are ordered from.
  
- Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly.
  - using the summary table above

```{r echo = TRUE}
instacart_summary %>%
  filter(items_sold > 10000) %>%
  mutate(aisle = factor(aisle, levels = aisle[order(sell_rank, decreasing = T)])) %>%
  ggplot(aes(x = aisle, y = items_sold)) +
    geom_bar(stat = "identity") +
    labs( x = "the selling aisle",
          y = "items that are sold") +
    coord_flip()
```

- Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in the table.
  - The data should:
    - restrict aisles that sells baking ingredients, dog food care or packaged vegetables fruits
    - keep 2 variables: product_name and aisle
    - create new variable called items_sold to summarize the total count of the items
    - create a new variable called sell_rank to show the most sold items in each aisle, in a descending order

```{r}
instacart_3pops = instacart %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle ==  "packaged vegetables fruits") %>%
  select(product_name, aisle) %>%
  group_by(aisle, product_name) %>%
  summarise(items_sold = n()) %>%
  mutate(sell_rank = min_rank(desc(items_sold))) %>%
  arrange(aisle, sell_rank) %>%
  filter(sell_rank <= 3)

kable(instacart_3pops)
```

- Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
  - the table should:
    - restrict product_name to pink lady apples and coffee ice cream
    - keep product name, the day of the order, and the time of the order
    - calculate the mean order time by day and product
    - apply weekday format to the day of the order, from 0-6 are Sunday to Saturday
    
```{r}
instacart_week = instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_order_time = mean(order_hour_of_day)) %>%
  mutate(order_dow = replace(order_dow, order_dow == 0, "Sunday"),
        order_dow = replace(order_dow, order_dow == 1, "Monday"), 
        order_dow = replace(order_dow, order_dow == 2, "Tuesday"), 
        order_dow = replace(order_dow, order_dow == 3, "Wednesday"),
        order_dow = replace(order_dow, order_dow == 4, "Thursday"),
        order_dow = replace(order_dow, order_dow == 5, "Friday"),
        order_dow = replace(order_dow, order_dow == 6, "Saturday")) %>%
  pivot_wider(names_from =  order_dow, values_from = mean_order_time)

kable(instacart_week)
```

# problem 2

- data importing and data cleaning
  - change locationabbr into state abbreviation
  - change locationdesc into county
  - keep the results of overall health topic
  - order the response from poor to excellent
  
```{r}
data("brfss_smart2010") 
brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  rename(state = locationabbr,
         location = locationdesc) %>%
  mutate(location = substr(location, 6, 40),
         response = factor(response, levels = c("Poor", "Fair","Good", "Very good", "Excellent"))) %>%
  arrange(response)
```

- In 2002, which states were observed at 7 or more locations? What about in 2010?
  - make a summary table that count the numbers of locations observed across the states in different years.

```{r}
brfss_loc = brfss %>%
  select(state, location, year) %>%
  filter(year == 2002 | year == 2010) %>%
  group_by(year, state) %>%
  summarize(n_loc = n()) %>%
  filter(n_loc > 7)

kable(brfss_loc)

```
From the table above, we can see that 36 states were observed at more than 7 locations in 2002. The 36 states are `r levels(as_factor(pull(subset(brfss_loc, year == 2002), state)))`. 45 states were observed at more than 7 locations in 2010, which are `r levels(as_factor(pull(subset(brfss_loc, year == 2010), state)))`. The number of observed location for each states for each year are shown above.

- make a table that summarizes the mean `data_value` in Excellent responses across year and state.

```{r}
brfss_excellent = brfss %>%
  filter(response == "Excellent") %>%
  select(year, state, data_value) %>%
  group_by(year,state) %>%
  summarise(mean_value = mean(data_value, na.rm = T))

brfss_excellent %>%
  ggplot(aes(x = year, y = mean_value, group = state)) +
    geom_line(aes(color = state))

```

- Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
  - the data for this plot should:
    - the data are restricted state as "NY", and restricted year to 2006 and 2010.
    - the data set contains 3 variables: year, responses (factor variable with 5 ordered levels), data_value.
    
```{r}
brfss_violin = brfss %>%
  filter((year == 2006 | year == 2010) & state == "NY") %>%
  select(year,location, response, data_value, confidence_limit_low, confidence_limit_high) %>%
  arrange(response) %>%
  ggplot(aes(x = response, y = data_value, fill = response)) +
    geom_violin() +
    labs(title = "distribution of `data_value` for responses among locations in NY State in 2006 and 2010",
         y = "prevalence(%)",
         x = "general health condition") +
    facet_wrap(~year)
brfss_violin  
```

# problem 3

- load and cleaning data
  - put `activity.*` into 1 column `time_min`, the original value of `activity.*` will be in a new variable called `activity_count`
  - create new variable `weekday` with 2 levels: "weekday" and "weekend", to show weekday vs. weekend
  - arranged by `day_id` and `time_min`
  - transform `time_min` to `time_hr` to approximate the real clock time
  
```{r}
acce = read_csv("./data/accel_data.csv",
                col_names = T) %>%
  pivot_longer(cols = starts_with("activity."),
               names_to = "time_min",
               names_prefix = "activity.",
               values_to = "activity_count") %>%
  mutate(weekday = if_else(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
         day = factor(day, levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")),
         time_min = as.numeric(time_min),
         time_hr = time_min/60) %>%
  arrange(day_id, time_min)
```

Now there are `r ncol(acce)` variables and `r nrow(acce)` observations in dataset `acce`. The 6 variables are: `r names(acce)`.  The activity counts ranged from `r min(pull(acce, activity_count))` to `r max(pull(acce, activity_count))` in the 5-week observation.

- make a summary table for total activity variable for each day. The table should be:
  - have 7 columns for each week day, and 5 rows for each week.
  - the total activity variable should be the value
  

```{r}
acce_totalac = acce %>%
  select(week, day, day_id, activity_count) %>%
  group_by(week, day) %>%
  summarise(total_activity_count = sum(activity_count, na.rm = T)) %>%
  pivot_wider(names_from = day,
              values_from = total_activity_count) %>%
  arrange(week) 

kable(acce_totalac)
```

- From the table above, the trends are: 
  - there is a decrease of total activity count in Tuesday and Saturday;
  - there is an increase of total activity count on Wednesday and Sunday.
  
- a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.
  - the data should be modified as:
    - including day, time_min, activity_count
  - the line plot is hard to distinguish the pattern, so add a `geom_smooth()` to show the pattern

```{r}
acce_daychange = acce %>%
  select(day_id, day, time_hr, activity_count) %>%
  group_by(day, day_id) %>%
  ggplot(aes(x = time_hr, y = activity_count, color = day)) +
    geom_line(aes(group = day_id),alpha = 0.4, size = 0.5) +
    geom_smooth(alpha = 0.8, size = 2) +
    scale_x_continuous(limits = c(0,24), breaks = c(0,4,8,12,16,20,24)) +
    scale_fill_viridis_c() +
    labs(title = "activity over the course of the day",
         x = "time in a day(hr), start from 00:00 to 23:59",
         y = "activity count") +
    theme_minimal()
acce_daychange
```

- From the plot, the patterns are:
  - The patient's activity is the lowest from 11 pm to 3 am  for all the weekdays.
  - On Monday, the patient is most active during around 10-11 am.
  - The patient is most active on Friday night.
  - Generally, the patient is least active on Saturday.


